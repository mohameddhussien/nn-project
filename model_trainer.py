# -*- coding: utf-8 -*-
"""nn-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iaai_mzghl4Iw1Y1S2FErSUjdzmhV9Jh
"""
import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from typing import Literal, Union


"""# Single Layer Perceptron / Adaptive Linear Neuron (Adaline) Training

## Initialize weights and bias

Initialize the weight vector `w` and bias `b` with random values or specific values (currently 1.5) using NumPy.
"""

def initialize_weights_and_bias(x, random: bool = False):
  if random:
    w = np.random.rand(x.shape[1])
    b = np.random.rand(1)[0]
    return w, b

  w = np.ones(x.shape[1]) * 1.5
  b = 0
  return w, b

"""## Define the activation function (the `signum` function)

"""

signum = lambda x: 1 if x >= 0 else -1

"""## Implement the training loop with defining a criterion

Determine when to stop the training process (ex: when no misclassifications occur in an epoch).

"""

def train(
    _x_train: np.array,
    _y_train: np.array,
    _w: np.array,
    _b: np.array,
    algorithm: Literal['perceptron', 'adaline'] = 'perceptron',
    epochs: int = 5,
    learning_rate: float = 0.01,
    threshold: Union[int, float] = 0
):
  errors, mse_values = [], []

  if (algorithm := algorithm.lower()) not in ['perceptron', 'adaline']:
    raise ValueError("Invalid algorithm. Use 'perceptron' or 'adaline'.")

  print(f"Training using ({algorithm.title()}) algorithm")

  for epoch in range(epochs):
    n_errors, epoch_mse = 0, 0

    for _x, _label in zip(_x_train, _y_train):
      linear_output = np.dot(_x, _w) + _b

      if algorithm == 'perceptron':
        y = signum(linear_output)
        target_signum = 1 if _label == 1 else -1

        if target_signum == y:
          continue

        update = learning_rate * (target_signum - y)
        _w += update * _x
        _b += update
        n_errors += 1
      elif algorithm == 'adaline':
        y = linear_output # linear activation
        update = learning_rate * (_label - y)
        _w += update * _x
        _b += update
        epoch_mse += (_label - y)**2 # mean squared error (MSE)

    # Define stopping criterion for each algorithm
    if algorithm == 'perceptron':
      errors.append(n_errors)
      if n_errors == threshold:
        print(f"{algorithm.title()} training converged at epoch {epoch + 1}")
        break
    elif algorithm == 'adaline':
      mse_values.append(mse := epoch_mse / len(_x_train))
      print(f"Epoch {epoch + 1}, MSE: {mse:.4f}")
      if mse < threshold:
        print(f"{algorithm.title()} training converged at epoch {epoch + 1}")
        break


  print("Training complete.")
  if algorithm == 'perceptron':
    return errors, (_w, _b)
  elif algorithm == 'adaline':
    return mse_values, (_w, _b)
  raise ValueError(f"Invalid algorithm = {algorithm.title()}. Use 'perceptron' or 'adaline'.")



def preprocess_data(file_path, features, classes):
    """
    Loads and preprocesses the penguin dataset.
    Returns x_train_scaled, y_train ready for model training.
    """

    data: pd.DataFrame = pd.read_csv(file_path)
    target_column = 'Species'

    features_and_target = features + [target_column]

    data = data[features_and_target]

    data = data[data[target_column].isin(classes)]
    data[target_column] = data[target_column].astype('category')

    # filling by mean
    _mean = data.mean(numeric_only=True)
    data = data.fillna(_mean)

    # since target model is adaline or perceptron (Linear models)
    # therefore one hot encoding is best
    # OriginLocation
    # our target column is 'Species' we will make to it labelencoder
    # 0 for 'Adelie' or 1 for 'Gentoo' or 2 for 'Chinstrap'
    categorical_columns = []
    if 'OriginLocation' in data.columns:
      data['OriginLocation'] = data['OriginLocation'].astype('category')
      categorical_columns.append('OriginLocation')

    # changed this without dropping
    data_encoded = data.loc[:, data.columns != target_column]

    if categorical_columns:
      enc = OneHotEncoder(sparse_output=False)
      encoding = enc.fit_transform(data[categorical_columns])
      cat_feature_names = enc.get_feature_names_out(categorical_columns)

      encoding = pd.DataFrame(encoding, columns=cat_feature_names, index=data_encoded.index)
      data_encoded = data_encoded.drop(columns=categorical_columns)
      data_encoded = pd.concat([data_encoded, encoding], axis=1)

    le = LabelEncoder()
    y_enc = le.fit_transform(data[target_column])

    """# Splitting data to train and test samples

    ### Note on `test_size` Parameter.
    Changed `test_size` from **0.2** to **0.4**. This adjustment is based on the requirement that each sample class should have 30 samples for training and 20 for testing.

    Given that each class has 50 samples total and we are using 2 classes:
    * **Total Training:** 30 samples/class * 2 classes = 60 samples
    * **Total Testing:** 20 samples/class * 2 classes = 40 samples
    * **Total Dataset:** 50 samples/class * 2 classes = 100 samples

    This results in a testing ratio of 40 / 100, so `test_size` is set to **0.4**.
    """

    x = data_encoded
    y = y_enc

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)

    scaler = StandardScaler()
    x_train_scaled = scaler.fit_transform(x_train)
    x_test_scaled = scaler.transform(x_test)

    return data, x_train_scaled, x_test_scaled, y_train, y_test, le


if __name__ == "__main__":
    # Run original standalone workflow for manual testing
    file_path = os.path.join(os.path.dirname(__file__), "_resources", "penguins.csv")
    # x_train_scaled, y_train = preprocess_data(file_path)
    print("Data successfully preprocessed for standalone use.")

